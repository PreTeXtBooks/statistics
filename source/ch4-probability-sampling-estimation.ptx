<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-probability-sampling-estimation">
  <title>Probability, Sampling, and Estimation</title>
  
  <introduction>
    <p>Authors: Danielle J. Navarro and Matthew J. C. Crump</p>
    <p>Chapter notes: Sections 4.1 &amp; 4.9 - Adapted text by Danielle Navarro. Section 4.10 - 4.11 &amp; 4.13 - Mix of Matthew Crump &amp; Danielle Navarro. Section 4.12 - 4.13 - Adapted text by Danielle Navarro</p>
    <blockquote>
      <p>
        I have studied many languages-French, Spanish and a little Italian, but no one told me that Statistics was a foreign language. <mdash />Charmaine J. Forde
      </p>
    </blockquote>

    <p>
      Up to this point in the book, we've discussed some of the key ideas in experimental design, and we've talked a little about how you can summarize a data set. To a lot of people, this is all there is to statistics: it's about calculating averages, collecting all the numbers, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting, but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics, and one of the least powerful. The bigger and more useful part of statistics is that it provides tools <em>that let you make inferences about data</em>.
    </p>

    <p>
      Once you start thinking about statistics in these terms <mdash /> that statistics is there to help us draw inferences from data <mdash /> you start seeing examples of it everywhere. For instance, here's a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):
    </p>

    <blockquote>
      <p>
        "I have a tough job," the Premier said in response to a poll which found her government is now the most unpopular Labor administration in polling history, with a primary vote of just 23 per cent.
      </p>
    </blockquote>

    <p>
      This kind of remark is entirely unremarkable in the papers or in everyday life, but let's have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I'm too lazy to track down the original survey, so let's just imagine that they called 1000 voters at random, and 230 (23%) of those claimed that they intended to vote for the party. For the 2010 Federal election, the Australian Electoral Commission reported 4,610,795 enrolled voters in New South Whales; so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?
    </p>

    <p>
      The answer to the question is pretty obvious: if I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the <em>only</em> 230 people out of the entire voting public who actually intend to do so. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it's a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.
    </p>

    <p>
      <term>Inferential statistics</term> provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, our tools for making statistical inferences are 1) built on top of <term>probability theory</term>, and 2) require an understanding of how samples behave when you take them from distributions (defined by probability theory...). So, this chapter has two main parts. A brief introduction to probability theory, and an introduction to sampling from distributions.
    </p>
  </introduction>

  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-probability-statistics-different.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-probability-mean.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-basic-probability-theory.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-binomial-distribution.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-normal-distribution.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-other-distributions.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-probability-summary.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-samples-populations-sampling.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-law-large-numbers.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-sampling-distributions-clt.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-central-limit-theorem.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-z-scores.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-estimating-parameters.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-confidence-intervals.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-sampling-estimation-summary.ptx" />
  <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="sec-videos-ch4.ptx" />

</chapter>
