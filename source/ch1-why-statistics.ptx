<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch1-why-statistics">
  <title>Why Statistics?</title>
        <introduction>
<p>Author: Danielle J. Navarro</p>
<p>Chapter notes: Adapted nearly verbatim from Chapters 1 and 2 in Navarro, D. J. "Learning Statistics with R." https://compcogscisydney.org/learning-statistics-with-r/</p>
  <blockquote>
    <p>
      To call in statisticians after the experiment is done may be no more than asking them to perform a post-mortem examination: They may be able to say what the experiment died of. <mdash />Sir Ronald Fisher
    </p>
  </blockquote>

      </introduction>
  <section xml:id="subsec-psychology-of-statistics">
    <title>On the psychology of statistics</title>
    
    <introduction>
      <p>
        To the surprise of many students, statistics is a fairly significant part of a psychological education. To the surprise of no-one, statistics is very rarely the <em>favorite</em> part of one's psychological education. After all, if you really loved the idea of doing statistics, you'd probably be enrolled in a statistics class right now, not a psychology class. So, not surprisingly, there's a pretty large proportion of the student base that isn't happy about the fact that psychology has so much statistics in it. In view of this, I thought that the right place to start might be to answer some of the more common questions that people have about stats...
      </p>

      <p>
        A big part of this issue at hand relates to the very idea of statistics. What is it? What's it there for? And why are scientists so bloody obsessed with it? These are all good questions, when you think about it. So let's start with the last one. As a group, scientists seem to be bizarrely fixated on running statistical tests on everything. In fact, we use statistics so often that we sometimes forget to explain to people why we do. It's a kind of article of faith among scientists <mdash /> and especially social scientists <mdash /> that your findings can't be trusted until you've done some stats. Undergraduate students might be forgiven for thinking that we're all completely mad, because no-one takes the time to answer one very simple question:
      </p>

      <blockquote>
        <p>
          <em>Why do you do statistics? Why don't scientists just use common sense?</em>
        </p>
      </blockquote>

      <p>
        It's a naive question in some ways, but most good questions are. There's a lot of good answers to it, but for my money, the best answer is a really simple one: we don't trust ourselves enough. We worry that we're human, and susceptible to all of the biases, temptations and frailties that humans suffer from. Much of statistics is basically a safeguard. Using "common sense" to evaluate evidence means trusting gut instincts, relying on verbal arguments and on using the raw power of human reason to come up with the right answer. Most scientists don't think this approach is likely to work.
      </p>

      <p>
        In fact, come to think of it, this sounds a lot like a psychological question to me, and since I do work in a psychology department, it seems like a good idea to dig a little deeper here. Is it really plausible to think that this "common sense" approach is very trustworthy? Verbal arguments have to be constructed in language, and all languages have biases <mdash /> some things are harder to say than others, and not necessarily because they're false (e.g., quantum electrodynamics is a good theory, but hard to explain in words). The instincts of our "gut" aren't designed to solve scientific problems, they're designed to handle day to day inferences <mdash /> and given that biological evolution is slower than cultural change, we should say that they're designed to solve the day to day problems for a <em>different world</em> than the one we live in. Most fundamentally, reasoning sensibly requires people to engage in "induction", making wise guesses and going beyond the immediate evidence of the senses to make generalizations about the world. If you think that you can do that without being influenced by various distractors, well, I have a bridge in Brooklyn I'd like to sell you. Heck, as the next section shows, we can't even solve "deductive" problems (ones where no guessing is required) without being influenced by our pre-existing biases.
      </p>
    </introduction>

    <subsection xml:id="subsubsec-belief-bias">
      <title>The curse of belief bias</title>
      
      <p>
        People are mostly pretty smart. We're certainly smarter than the other species that we share the planet with (though many people might disagree). Our minds are quite amazing things, and we seem to be capable of the most incredible feats of thought and reason. That doesn't make us perfect though. And among the many things that psychologists have shown over the years is that we really do find it hard to be neutral, to evaluate evidence impartially and without being swayed by pre-existing biases. A good example of this is the <term>belief bias effect</term> in logical reasoning: if you ask people to decide whether a particular argument is logically valid (i.e., conclusion would be true if the premises were true), we tend to be influenced by the believability of the conclusion, even when we shouldn't. For instance, here's a valid argument where the conclusion is believable:
      </p>

      <ul>
        <li><p>No cigarettes are inexpensive (Premise 1)</p></li>
        <li><p>Some addictive things are inexpensive (Premise 2)</p></li>
        <li><p>Therefore, some addictive things are not cigarettes (Conclusion)</p></li>
      </ul>

      <p>
        And here's a valid argument where the conclusion is not believable:
      </p>

      <ul>
        <li><p>No addictive things are inexpensive (Premise 1)</p></li>
        <li><p>Some cigarettes are inexpensive (Premise 2)</p></li>
        <li><p>Therefore, some cigarettes are not addictive (Conclusion)</p></li>
      </ul>

      <p>
        The logical <em>structure</em> of argument #2 is identical to the structure of argument #1, and they're both valid. However, in the second argument, there are good reasons to think that premise 1 is incorrect, and as a result it's probably the case that the conclusion is also incorrect. But that's entirely irrelevant to the topic at hand: an argument is deductively valid if the conclusion is a logical consequence of the premises. That is, a valid argument doesn't have to involve true statements.
      </p>

      <p>
        On the other hand, here's an invalid argument that has a believable conclusion:
      </p>

      <ul>
        <li><p>No addictive things are inexpensive (Premise 1)</p></li>
        <li><p>Some cigarettes are inexpensive (Premise 2)</p></li>
        <li><p>Therefore, some addictive things are not cigarettes (Conclusion)</p></li>
      </ul>

      <p>
        And finally, an invalid argument with an unbelievable conclusion:
      </p>

      <ul>
        <li><p>No cigarettes are inexpensive (Premise 1)</p></li>
        <li><p>Some addictive things are inexpensive (Premise 2)</p></li>
        <li><p>Therefore, some cigarettes are not addictive (Conclusion)</p></li>
      </ul>

      <p>
        Now, suppose that people really are perfectly able to set aside their pre-existing biases about what is true and what isn't, and purely evaluate an argument on its logical merits. We'd expect 100% of people to say that the valid arguments are valid, and 0% of people to say that the invalid arguments are valid. So if you ran an experiment looking at this, you'd expect to see data like this:
      </p>

      <table xml:id="table-belief-bias-expected">
        <title>Expected results if people could set aside biases</title>
        <tabular>
          <row header="yes">
            <cell></cell>
            <cell>conclusion feels true</cell>
            <cell>conclusion feels false</cell>
          </row>
          <row>
            <cell>argument is valid</cell>
            <cell>100% say "valid"</cell>
            <cell>100% say "valid"</cell>
          </row>
          <row>
            <cell>argument is invalid</cell>
            <cell>0% say "valid"</cell>
            <cell>0% say "valid"</cell>
          </row>
        </tabular>
      </table>

      <p>
        If the psychological data looked like this (or even a good approximation to this), we might feel safe in just trusting our gut instincts. That is, it'd be perfectly okay just to let scientists evaluate data based on their common sense, and not bother with all this murky statistics stuff. However, you guys have taken psych classes, and by now you probably know where this is going.
      </p>

      <p>
        In a classic study, Evans, Barston and Pollard (1983) ran an experiment looking at exactly this. What they found is that when pre-existing biases (i.e., beliefs) were in agreement with the structure of the data, everything went the way you'd hope:
      </p>

      <table xml:id="table-belief-bias-agreement">
        <title>Results when biases agree with structure</title>
        <tabular>
          <row header="yes">
            <cell></cell>
            <cell>conclusion feels true</cell>
            <cell>conclusion feels false</cell>
          </row>
          <row>
            <cell>argument is valid</cell>
            <cell>92% say "valid"</cell>
            <cell><mdash /></cell>
          </row>
          <row>
            <cell>argument is invalid</cell>
            <cell><mdash /></cell>
            <cell>8% say "valid"</cell>
          </row>
        </tabular>
      </table>

      <p>
        Not perfect, but that's pretty good. But look what happens when our intuitive feelings about the truth of the conclusion run against the logical structure of the argument:
      </p>

      <table xml:id="table-belief-bias-conflict">
        <title>Results when biases conflict with structure</title>
        <tabular>
          <row header="yes">
            <cell></cell>
            <cell>conclusion feels true</cell>
            <cell>conclusion feels false</cell>
          </row>
          <row>
            <cell>argument is valid</cell>
            <cell>92% say "valid"</cell>
            <cell><alert>46% say "valid"</alert></cell>
          </row>
          <row>
            <cell>argument is invalid</cell>
            <cell><alert>92% say "valid"</alert></cell>
            <cell>8% say "valid"</cell>
          </row>
        </tabular>
      </table>

      <p>
        Oh dear, that's not as good. Apparently, when people are presented with a strong argument that contradicts our pre-existing beliefs, we find it pretty hard to even perceive it to be a strong argument (people only did so 46% of the time). Even worse, when people are presented with a weak argument that agrees with our pre-existing biases, almost no-one can see that the argument is weak (people got that one wrong 92% of the time!)
      </p>

      <p>
        If you think about it, it's not as if these data are horribly damning. Overall, people did do better than chance at compensating for their prior biases, since about 60% of people's judgement were correct (you'd expect 50% by chance). Even so, if you were a professional "evaluator of evidence", and someone came along and offered you a magic tool that improves your chances of making the right decision from 60% to (say) 95%, you'd probably jump at it, right? Of course you would. Thankfully, we actually do have a tool that can do this. But it's not magic, it's statistics. So that's reason #1 why scientists love statistics. It's just <em>too easy</em> for us to "believe what we want to believe"; so if we want to "believe in the data" instead, we're going to need a bit of help to keep our personal biases under control. That's what statistics does: it helps keep us honest.
      </p>
    </subsection>
  </section>

  <section xml:id="subsec-simpsons-paradox">
    <title>The cautionary tale of Simpson's paradox</title>
    
    <p>
      The following is a true story (I think...). In 1973, the University of California, Berkeley had some worries about the admissions of students into their postgraduate courses. Specifically, the thing that caused the problem was that the gender breakdown of their admissions, which looked like this:
    </p>

    <table xml:id="table-berkeley-overall">
      <title>Berkeley 1973 admissions data by gender</title>
      <tabular>
        <row header="yes">
          <cell></cell>
          <cell>Number of applicants</cell>
          <cell>Percent admitted</cell>
        </row>
        <row>
          <cell>Males</cell>
          <cell>8442</cell>
          <cell>44%</cell>
        </row>
        <row>
          <cell>Females</cell>
          <cell>4321</cell>
          <cell>35%</cell>
        </row>
      </tabular>
    </table>

    <p>
      and they were worried about being sued. Given that there were nearly 13,000 applicants, a difference of 9% in admission rates between males and females is just way too big to be a coincidence. Pretty compelling data, right? And if I were to say to you that these data <em>actually</em> reflect a weak bias in favor of women (sort of!), you'd probably think that I was either crazy or sexist.
    </p>

    <note>
      <title>Extra</title>
      <p>
        Earlier versions of these notes incorrectly suggested that they actually were sued <mdash /> apparently that's not true. There's a nice commentary on this here: <url href="https://www.refsmmat.com/posts/2016-05-08-simpsons-paradox-berkeley.html" visual="refsmmat.com/posts/2016-05-08-simpsons-paradox-berkeley.html">https://www.refsmmat.com/posts/2016-05-08-simpsons-paradox-berkeley.html</url>. A big thank you to Wilfried Van Hirtum for pointing this out to me!
      </p>
    </note>

    <p>
      When people started looking more carefully at the admissions data (Bickel, Hammel, and O'Connell, 1975) they told a rather different story. Specifically, when they looked at it on a department by department basis, it turned out that most of the departments actually had a slightly <em>higher</em> success rate for female applicants than for male applicants. The table below shows the admission figures for the six largest departments (with the names of the departments removed for privacy reasons):
    </p>

    <table xml:id="table-berkeley-by-dept">
      <title>Berkeley 1973 admissions data by department</title>
      <tabular>
        <row header="yes">
          <cell>Department</cell>
          <cell>Male Applicants</cell>
          <cell>Male % admitted</cell>
          <cell>Female Applicants</cell>
          <cell>Female % admitted</cell>
        </row>
        <row>
          <cell>A</cell>
          <cell>825</cell>
          <cell>62%</cell>
          <cell>108</cell>
          <cell>82%</cell>
        </row>
        <row>
          <cell>B</cell>
          <cell>560</cell>
          <cell>63%</cell>
          <cell>25</cell>
          <cell>68%</cell>
        </row>
        <row>
          <cell>C</cell>
          <cell>325</cell>
          <cell>37%</cell>
          <cell>593</cell>
          <cell>34%</cell>
        </row>
        <row>
          <cell>D</cell>
          <cell>417</cell>
          <cell>33%</cell>
          <cell>375</cell>
          <cell>35%</cell>
        </row>
        <row>
          <cell>E</cell>
          <cell>191</cell>
          <cell>28%</cell>
          <cell>393</cell>
          <cell>24%</cell>
        </row>
        <row>
          <cell>F</cell>
          <cell>272</cell>
          <cell>6%</cell>
          <cell>341</cell>
          <cell>7%</cell>
        </row>
      </tabular>
    </table>

    <p>
      Remarkably, most departments had a <em>higher</em> rate of admissions for females than for males! Yet the overall rate of admission across the university for females was <em>lower</em> than for males. How can this be? How can both of these statements be true at the same time?
    </p>

    <p>
      Here's what's going on. Firstly, notice that the departments are <em>not</em> equal to one another in terms of their admission percentages: some departments (e.g., engineering, chemistry) tended to admit a high percentage of the qualified applicants, whereas others (e.g., English) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get <alert>A</alert><m>&gt;</m><alert>B</alert><m>&gt;</m>D<m>&gt;</m>C<m>&gt;</m>F<m>&gt;</m>E (the "easy" departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C<m>&gt;</m>E<m>&gt;</m>D<m>&gt;</m>F<m>&gt;</m><alert>A</alert><m>&gt;</m><alert>B</alert>. In other words, what these data seem to be suggesting is that the female applicants tended to apply to "harder" departments. And in fact, if we look at <xref ref="fig-simpson" /> we see that this trend is systematic, and quite striking. This effect is known as <term>Simpson's paradox</term>. It's not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it's real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point ...doing research is hard, and there are <em>lots</em> of subtle, counter intuitive traps lying in wait for the unwary. That's reason #2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.
    </p>

    <figure xml:id="fig-simpson">
      <caption>The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from Bickel et al. (1975). Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot department with fewer than 40 applicants.</caption>
      <image source="imgs/figures/1Simpson.png" width="80%">
        <description>Scatter plot showing the relationship between percentage of female applicants and admission rates across Berkeley departments in 1973</description>
      </image>
    </figure>

    <p>
      Before leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves <em>part</em> of the problem. Remember that we started all this with the concern that Berkeley's admissions processes might be unfairly biased against female applicants. When we looked at the "aggregated" data, it did seem like the university was discriminating against women, but when we "disaggregate" and looked at the individual behavior of all the departments, it turned out that the actual departments were, if anything, slightly biased in favor of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments. From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department (and there are good reasons to do that), and at the level of individual departments, the decisions are more or less unbiased (the weak bias in favor of females at that level is small, and not consistent across departments). Since the university can't dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce.
    </p>

    <p>
      That was the basis for my somewhat glib remarks earlier, but that's not exactly the whole story, is it? After all, if we're interested in this from a more sociological and psychological perspective, we might want to ask <em>why</em> there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to "hard sciences" and females prefer "humanities". And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn't want to fund the humanities (spots in Ph.D. programs, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are "useless chick stuff". That seems pretty <em>blatantly</em> gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you're interested in the overall structural effects of subtle gender biases, then you probably want to look at <em>both</em> the aggregated and disaggregated data. If you're interested in the decision making process at Berkeley itself then you're probably only interested in the disaggregated data.
    </p>

    <p>
      In short there are a lot of critical questions that you can't answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a <em>tool</em> to help you learn about your data, no more and no less. It's a powerful tool to that end, but there's no substitute for careful thought.
    </p>
  </section>

  <section xml:id="subsec-statistics-in-psychology">
    <title>Statistics in psychology</title>
    
    <p>
      I hope that the discussion above helped explain why science in general is so focused on statistics. But I'm guessing that you have a lot more questions about what role statistics plays in psychology, and specifically why psychology classes always devote so many lectures to stats. So here's my attempt to answer a few of them...
    </p>

    <ul>
      <li>
        <p><term>Why does psychology have so much statistics?</term></p>
        <p>
          To be perfectly honest, there's a few different reasons, some of which are better than others. The most important reason is that psychology is a statistical science. What I mean by that is that the "things" that we study are <em>people</em>. Real, complicated, gloriously messy, infuriatingly perverse people. The "things" of physics include objects like electrons, and while there are all sorts of complexities that arise in physics, electrons don't have minds of their own. They don't have opinions, they don't differ from each other in weird and arbitrary ways, they don't get bored in the middle of an experiment, and they don't get angry at the experimenter and then deliberately try to sabotage the data set. At a fundamental level psychology is harder than physics.
        </p>
        <p>
          Basically, we teach statistics to you as psychologists because you need to be better at stats than physicists. There's actually a saying used sometimes in physics, to the effect that "if your experiment needs statistics, you should have done a better experiment". They have the luxury of being able to say that because their objects of study are pathetically simple in comparison to the vast mess that confronts social scientists. It's not just psychology, really: most social sciences are desperately reliant on statistics. Not because we're bad experimenters, but because we've picked a harder problem to solve. We teach you stats because you really, really need it.
        </p>
      </li>

      <li>
        <p><term>Can't someone else do the statistics?</term></p>
        <p>
          To some extent, but not completely. It's true that you don't need to become a fully trained statistician just to do psychology, but you do need to reach a certain level of statistical competence. In my view, there's three reasons that every psychological researcher ought to be able to do basic statistics:
        </p>
        <ol>
          <li><p>There's the fundamental reason: statistics is deeply intertwined with research design. If you want to be good at designing psychological studies, you need to at least understand the basics of stats.</p></li>
          <li><p>If you want to be good at the psychological side of the research, then you need to be able to understand the psychological literature, right? But almost every paper in the psychological literature reports the results of statistical analyses. So if you really want to understand the psychology, you need to be able to understand what other people did with their data. And that means understanding a certain amount of statistics.</p></li>
          <li><p>There's a big practical problem with being dependent on other people to do all your statistics: statistical analysis is <em>expensive</em>. In almost any real life situation where you want to do psychological research, the cruel facts will be that you don't have enough money to afford a statistician. So the economics of the situation mean that you have to be pretty self-sufficient.</p></li>
        </ol>
        <p>
          Note that a lot of these reasons generalize beyond researchers. If you want to be a practicing psychologist and stay on top of the field, it helps to be able to read the scientific literature, which relies pretty heavily on statistics.
        </p>
      </li>

      <li>
        <p><term>I don't care about jobs, research, or clinical work. Do I need statistics?</term></p>
        <p>
          Okay, now you're just messing with me. Still, I think it should matter to you too. Statistics should matter to you in the same way that statistics should matter to <em>everyone</em>: we live in the 21st century, and data are <em>everywhere</em>. Frankly, given the world in which we live these days, a basic knowledge of statistics is pretty damn close to a survival tool! Which is the topic of the next section...
        </p>
      </li>
    </ul>
  </section>

  <section xml:id="subsec-statistics-in-everyday-life">
    <title>Statistics in everyday life</title>
    
    <blockquote>
      <p>
        <q>We are drowning in information, but we are starved for knowledge</q>
      </p>
      <p>
        <mdash /> Various authors, original probably John Naisbitt
      </p>
    </blockquote>

    <p>
      When I started writing up my lecture notes I took the 20 most recent news articles posted to the ABC news website. Of those 20 articles, it turned out that 8 of them involved a discussion of something that I would call a statistical topic; 6 of those made a mistake. The most common error, if you're curious, was failing to report baseline data (e.g., the article mentions that 5% of people in situation X have some characteristic Y, but doesn't say how common the characteristic is for everyone else!) The point I'm trying to make here isn't that journalists are bad at statistics (though they almost always are), it's that a basic knowledge of statistics is very helpful for trying to figure out when someone else is either making a mistake or even lying to you. Perhaps, one of the biggest things that a knowledge of statistics does to you is cause you to get angry at the newspaper or the internet on a far more frequent basis :).
    </p>
  </section>

  <section xml:id="subsec-more-than-statistics">
    <title>There's more to research methods than statistics</title>
    
    <p>
      So far, most of what I've talked about is statistics, and so you'd be forgiven for thinking that statistics is all I care about in life. To be fair, you wouldn't be far wrong, but research methodology is a broader concept than statistics. So most research methods courses will cover a lot of topics that relate much more to the pragmatics of research design, and in particular the issues that you encounter when trying to do research with humans. However, about 99% of student <em>fears</em> relate to the statistics part of the course, so I've focused on the stats in this discussion, and hopefully I've convinced you that statistics matters, and more importantly, that it's not to be feared. That being said, it's pretty typical for introductory research methods classes to be very stats-heavy. This is not (usually) because the lecturers are evil people. Quite the contrary, in fact. Introductory classes focus a lot on the statistics because you almost always find yourself needing statistics before you need the other research methods training. Why? Because almost all of your assignments in other classes will rely on statistical training, to a much greater extent than they rely on other methodological tools. It's not common for undergraduate assignments to require you to design your own study from the ground up (in which case you would need to know a lot about research design), but it <em>is</em> common for assignments to ask you to analyse and interpret data that were collected in a study that someone else designed (in which case you need statistics). In that sense, from the perspective of allowing you to do well in all your other classes, the statistics is more urgent.
    </p>

    <p>
      But note that "urgent" is different from "important" <mdash /> they both matter. I really do want to stress that research design is just as important as data analysis, and this book does spend a fair amount of time on it. However, while statistics has a kind of universality, and provides a set of core tools that are useful for most types of psychological research, the research methods side isn't quite so universal. There are some general principles that everyone should think about, but a lot of research design is very idiosyncratic, and is specific to the area of research that you want to engage in. To the extent that it's the details that matter, those details don't usually show up in an introductory stats and research methods class.
    </p>
  </section>

  <section xml:id="subsec-research-design-intro">
    <title>A brief introduction to research design</title>
    
    <p>
      In this chapter, we're going to start thinking about the basic ideas that go into designing a study, collecting data, checking whether your data collection works, and so on. It won't give you enough information to allow you to design studies of your own, but it will give you a lot of the basic tools that you need to assess the studies done by other people. However, since the focus of this book is much more on data analysis than on data collection, I'm only giving a very brief overview. Note that this chapter is "special" in two ways. Firstly, it's much more psychology-specific than the later chapters. Secondly, it focuses much more heavily on the scientific problem of research methodology, and much less on the statistical problem of data analysis. Nevertheless, the two problems are related to one another, so it's traditional for stats textbooks to discuss the problem in a little detail. This chapter relies heavily on Campbell and Stanley (1963) for the discussion of study design, and Stevens (1946) for the discussion of scales of measurement. Later versions will attempt to be more precise in the citations.
    </p>
  </section>

  <section xml:id="subsec-psychological-measurement">
    <title>Introduction to psychological measurement</title>
    
    <introduction>
      <p>
        The first thing to understand is data collection can be thought of as a kind of <term>measurement</term>. That is, what we're trying to do here is measure something about human behavior or the human mind. What do I mean by "measurement"?
      </p>
    </introduction>

    <subsection xml:id="subsubsec-thoughts-on-measurement">
      <title>Some thoughts about psychological measurement</title>
      
      <p>
        Measurement itself is a subtle concept, but basically it comes down to finding some way of assigning numbers, or labels, or some other kind of well-defined descriptions to "stuff". So, any of the following would count as a psychological measurement:
      </p>

      <ul>
        <li><p>My <term>age</term> is <em>33 years</em>.</p></li>
        <li><p>I <em>do not</em> <term>like anchovies</term>.</p></li>
        <li><p>My <term>chromosomal gender</term> is <em>male</em>.</p></li>
        <li><p>My <term>self-identified gender</term> is <em>male</em>.</p></li>
      </ul>

      <p>
        In the short list above, the bolded part is "the thing to be measured", and the italicized part is "the measurement itself". In fact, we can expand on this a little bit, by thinking about the set of possible measurements that could have arisen in each case:
      </p>

      <ul>
        <li><p>My <term>age</term> (in years) could have been <em>0, 1, 2, 3 ...</em>, etc. The upper bound on what my age could possibly be is a bit fuzzy, but in practice you'd be safe in saying that the largest possible age is <em>150</em>, since no human has ever lived that long.</p></li>
        <li><p>When asked if I <term>like anchovies</term>, I might have said that <em>I do</em>, or <em>I do not</em>, or <em>I have no opinion</em>, or <em>I sometimes do</em>.</p></li>
        <li><p>My <term>chromosomal gender</term> is almost certainly going to be <em>male (XY)</em> or <em>female (XX)</em>, but there are a few other possibilities. I could also have <em>Klinfelter's syndrome (XXY)</em>, which is more similar to male than to female. And I imagine there are other possibilities too.</p></li>
        <li><p>My <term>self-identified gender</term> is also very likely to be <em>male</em> or <em>female</em>, but it doesn't have to agree with my chromosomal gender. I may also choose to identify with <em>neither</em>, or to explicitly call myself <em>transgender</em>.</p></li>
      </ul>

      <p>
        As you can see, for some things (like age) it seems fairly obvious what the set of possible measurements should be, whereas for other things it gets a bit tricky. But I want to point out that even in the case of someone's age, it's much more subtle than this. For instance, in the example above, I assumed that it was okay to measure age in years. But if you're a developmental psychologist, that's way too crude, and so you often measure age in <em>years and months</em> (if a child is 2 years and 11 months, this is usually written as "2;11"). If you're interested in newborns, you might want to measure age in <em>days since birth</em>, maybe even <em>hours since birth</em>. In other words, the way in which you specify the allowable measurement values is important.
      </p>

      <p>
        Looking at this a bit more closely, you might also realize that the concept of "age" isn't actually all that precise. In general, when we say "age" we implicitly mean "the length of time since birth". But that's not always the right way to do it. Suppose you're interested in how newborn babies control their eye movements. If you're interested in kids that young, you might also start to worry that "birth" is not the only meaningful point in time to care about. If Baby Alice is born 3 weeks premature and Baby Bianca is born 1 week late, would it really make sense to say that they are the "same age" if we encountered them "2 hours after birth"? In one sense, yes: by social convention, we use birth as our reference point for talking about age in everyday life, since it defines the amount of time the person has been operating as an independent entity in the world, but from a scientific perspective that's not the only thing we care about. When we think about the biology of human beings, it's often useful to think of ourselves as organisms that have been growing and maturing since conception, and from that perspective Alice and Bianca aren't the same age at all. So you might want to define the concept of "age" in two different ways: the length of time since conception, and the length of time since birth. When dealing with adults, it won't make much difference, but when dealing with newborns it might.
      </p>

      <p>
        Moving beyond these issues, there's the question of methodology. What specific "measurement method" are you going to use to find out someone's age? As before, there are lots of different possibilities:
      </p>

      <ul>
        <li><p>You could just ask people "how old are you?" The method of self-report is fast, cheap and easy, but it only works with people old enough to understand the question, and some people lie about their age.</p></li>
        <li><p>You could ask an authority (e.g., a parent) "how old is your child?" This method is fast, and when dealing with kids it's not all that hard since the parent is almost always around. It doesn't work as well if you want to know "age since conception", since a lot of parents can't say for sure when conception took place. For that, you might need a different authority (e.g., an obstetrician).</p></li>
        <li><p>You could look up official records, like birth certificates. This is time consuming and annoying, but it has its uses (e.g., if the person is now dead).</p></li>
      </ul>
    </subsection>

    <subsection xml:id="subsubsec-operationalization">
      <title>Operationalization: defining your measurement</title>
      
      <p>
        All of the ideas discussed in the previous section all relate to the concept of <term>operationalization</term>. To be a bit more precise about the idea, operationalization is the process by which we take a meaningful but somewhat vague concept, and turn it into a precise measurement. The process of operationalization can involve several different things:
      </p>

      <ul>
        <li><p>Being precise about what you are trying to measure: For instance, does "age" mean "time since birth" or "time since conception" in the context of your research?</p></li>
        <li><p>Determining what method you will use to measure it: Will you use self-report to measure age, ask a parent, or look up an official record? If you're using self-report, how will you phrase the question?</p></li>
        <li><p>Defining the set of the allowable values that the measurement can take: Note that these values don't always have to be numerical, though they often are. When measuring age, the values are numerical, but we still need to think carefully about what numbers are allowed. Do we want age in years, years and months, days, hours? Etc. For other types of measurements (e.g., gender), the values aren't numerical. But, just as before, we need to think about what values are allowed. If we're asking people to self-report their gender, what options to we allow them to choose between? Is it enough to allow only "male" or "female"? Do you need an "other" option? Or should we not give people any specific options, and let them answer in their own words? And if you open up the set of possible values to include all verbal response, how will you interpret their answers?</p></li>
      </ul>

      <p>
        Operationalization is a tricky business, and there's no "one, true way" to do it. The way in which you choose to operationalize the informal concept of "age" or "gender" into a formal measurement depends on what you need to use the measurement for. Often you'll find that the community of scientists who work in your area have some fairly well-established ideas for how to go about it. In other words, operationalization needs to be thought through on a case by case basis. Nevertheless, while there a lot of issues that are specific to each individual research project, there are some aspects to it that are pretty general.
      </p>

      <p>
        Before moving on, I want to take a moment to clear up our terminology, and in the process introduce one more term. Here are four different things that are closely related to each other:
      </p>

      <ul>
        <li><p><term>A theoretical construct</term>. This is the thing that you're trying to take a measurement of, like "age", "gender" or an "opinion". A theoretical construct can't be directly observed, and often they're actually a bit vague.</p></li>
        <li><p><term>A measure</term>. The measure refers to the method or the tool that you use to make your observations. A question in a survey, a behavioral observation or a brain scan could all count as a measure.</p></li>
        <li><p><term>An operationalization</term>. The term "operationalization" refers to the logical connection between the measure and the theoretical construct, or to the process by which we try to derive a measure from a theoretical construct.</p></li>
        <li><p><term>A variable</term>. Finally, a new term. A variable is what we end up with when we apply our measure to something in the world. That is, variables are the actual "data" that we end up with in our data sets.</p></li>
      </ul>

      <p>
        In practice, even scientists tend to blur the distinction between these things, but it's very helpful to try to understand the differences.
      </p>
    </subsection>
  </section>

  <section xml:id="subsec-scales-of-measurement">
    <title>Scales of measurement</title>
    
    <p>
      As the previous section indicates, the outcome of a psychological measurement is called a variable. But not all variables are of the same qualitative type, and it's very useful to understand what types there are. A very useful concept for distinguishing between different types of variables is what's known as <term>scales of measurement</term>.
    </p>

    <subsection xml:id="subsubsec-nominal-scale">
      <title>Nominal scale</title>
      
      <p>
        A <term>nominal scale</term> variable (also referred to as a <term>categorical</term> variable) is one in which there is no particular relationship between the different possibilities: for these kinds of variables it doesn't make any sense to say that one of them is "bigger" or "better" than any other one, and it absolutely doesn't make any sense to average them. The classic example for this is "eye color". Eyes can be blue, green and brown, among other possibilities, but none of them is any "better" than any other one. As a result, it would feel really weird to talk about an "average eye color". Similarly, gender is nominal too: male isn't better or worse than female, neither does it make sense to try to talk about an "average gender". In short, nominal scale variables are those for which the only thing you can say about the different possibilities is that they are different. That's it.
      </p>

      <p>
        Let's take a slightly closer look at this. Suppose I was doing research on how people commute to and from work. One variable I would have to measure would be what kind of transportation people use to get to work. This "transport type" variable could have quite a few possible values, including: "train", "bus", "car", "bicycle", etc. For now, let's suppose that these four are the only possibilities, and suppose that when I ask 100 people how they got to work today, and I get this:
      </p>

      <table xml:id="table-transport-ordered">
        <title>Transportation types in order</title>
        <tabular>
          <row header="yes">
            <cell halign="left">Transportation</cell>
            <cell halign="center">Number of people</cell>
          </row>
          <row>
            <cell halign="left">(1) Train</cell>
            <cell halign="center">12</cell>
          </row>
          <row>
            <cell halign="left">(2) Bus</cell>
            <cell halign="center">30</cell>
          </row>
          <row>
            <cell halign="left">(3) Car</cell>
            <cell halign="center">48</cell>
          </row>
          <row>
            <cell halign="left">(4) Bicycle</cell>
            <cell halign="center">10</cell>
          </row>
        </tabular>
      </table>

      <p>
        So, what's the average transportation type? Obviously, the answer here is that there isn't one. It's a silly question to ask. You can say that travel by car is the most popular method, and travel by train is the least popular method, but that's about all. Similarly, notice that the order in which I list the options isn't very interesting. I could have chosen to display the data like this and nothing really changes.
      </p>

      <table xml:id="table-transport-reordered">
        <title>Transportation types in different order</title>
        <tabular>
          <row header="yes">
            <cell halign="left">Transportation</cell>
            <cell halign="center">Number of people</cell>
          </row>
          <row>
            <cell halign="left">(3) Car</cell>
            <cell halign="center">48</cell>
          </row>
          <row>
            <cell halign="left">(1) Train</cell>
            <cell halign="center">12</cell>
          </row>
          <row>
            <cell halign="left">(4) Bicycle</cell>
            <cell halign="center">10</cell>
          </row>
          <row>
            <cell halign="left">(2) Bus</cell>
            <cell halign="center">30</cell>
          </row>
        </tabular>
      </table>
    </subsection>

    <subsection xml:id="subsubsec-ordinal-scale">
      <title>Ordinal scale</title>
      
      <p>
        <term>Ordinal scale</term> variables have a bit more structure than nominal scale variables, but not by a lot. An ordinal scale variable is one in which there is a natural, meaningful way to order the different possibilities, but you can't do anything else. The usual example given of an ordinal variable is "finishing position in a race". You <em>can</em> say that the person who finished first was faster than the person who finished second, but you <em>don't</em> know how much faster. As a consequence we know that 1st <m>&gt;</m> 2nd, and we know that 2nd <m>&gt;</m> 3rd, but the difference between 1st and 2nd might be much larger than the difference between 2nd and 3rd.
      </p>

      <p>
        Here's an more psychologically interesting example. Suppose I'm interested in people's attitudes to climate change, and I ask them to pick one of these four statements that most closely matches their beliefs:
      </p>

      <blockquote>
        <p>
          (1) Temperatures are rising, because of human activity<br />
          (2) Temperatures are rising, but we don't know why<br />
          (3) Temperatures are rising, but not because of humans<br />
          (4) Temperatures are not rising
        </p>
      </blockquote>

      <p>
        Notice that these four statements actually do have a natural ordering, in terms of "the extent to which they agree with the current science". Statement 1 is a close match, statement 2 is a reasonable match, statement 3 isn't a very good match, and statement 4 is in strong opposition to the science. So, in terms of the thing I'm interested in (the extent to which people endorse the science), I can order the items as <m>1 &gt; 2 &gt; 3 &gt; 4</m>. Since this ordering exists, it would be very weird to list the options like this...
      </p>

      <blockquote>
        <p>
          (3) Temperatures are rising, but not because of humans<br />
          (4) Temperatures are rising, because of human activity<br />
          (5) Temperatures are not rising<br />
          (6) Temperatures are rising, but we don't know why
        </p>
      </blockquote>

      <p>
        ...because it seems to violate the natural "structure" to the question.
      </p>

      <p>
        So, let's suppose I asked 100 people these questions, and got the following answers:
      </p>

      <table xml:id="table-climate-responses">
        <title>Climate change attitude responses</title>
        <tabular>
          <row header="yes">
            <cell halign="left"></cell>
            <cell halign="center">Number</cell>
          </row>
          <row>
            <cell halign="left">(1) Temperatures are rising, because of human activity</cell>
            <cell halign="center">51</cell>
          </row>
          <row>
            <cell halign="left">(2) Temperatures are rising, but we don't know why</cell>
            <cell halign="center">20</cell>
          </row>
          <row>
            <cell halign="left">(3) Temperatures are rising, but not because of humans</cell>
            <cell halign="center">10</cell>
          </row>
          <row>
            <cell halign="left">(4) Temperatures are not rising</cell>
            <cell halign="center">19</cell>
          </row>
        </tabular>
      </table>

      <p>
        When analyzing these data, it seems quite reasonable to try to group (1), (2) and (3) together, and say that 81 of 100 people were willing to <em>at least partially</em> endorse the science. And it's <em>also</em> quite reasonable to group (2), (3) and (4) together and say that 49 of 100 people registered <em>at least some disagreement</em> with the dominant scientific view. However, it would be entirely bizarre to try to group (1), (2) and (4) together and say that 90 of 100 people said...what? There's nothing sensible that allows you to group those responses together at all.
      </p>

      <p>
        That said, notice that while we <em>can</em> use the natural ordering of these items to construct sensible groupings, what we <em>can't</em> do is average them. For instance, in my simple example here, the "average" response to the question is 1.97. If you can tell me what that means, I'd love to know. Because that sounds like gibberish to me!
      </p>
    </subsection>

    <subsection xml:id="subsubsec-interval-scale">
      <title>Interval scale</title>
      
      <p>
        In contrast to nominal and ordinal scale variables, <term>interval scale</term> and ratio scale variables are variables for which the numerical value is genuinely meaningful. In the case of interval scale variables, the <em>differences</em> between the numbers are interpretable, but the variable doesn't have a "natural" zero value. A good example of an interval scale variable is measuring temperature in degrees Celsius. For instance, if it was 15<m>^\circ</m> yesterday and 18<m>^\circ</m> today, then the 3<m>^\circ</m> difference between the two is genuinely meaningful. Moreover, that 3<m>^\circ</m> difference is <em>exactly the same</em> as the 3<m>^\circ</m> difference between <m>7^\circ</m> and <m>10^\circ</m>. In short, addition and subtraction are meaningful for interval scale variables.
      </p>

      <p>
        However, notice that the <m>0^\circ</m> does not mean "no temperature at all": it actually means "the temperature at which water freezes", which is pretty arbitrary. As a consequence, it becomes pointless to try to multiply and divide temperatures. It is wrong to say that <m>20^\circ</m> is <em>twice as hot</em> as <m>10^\circ</m>, just as it is weird and meaningless to try to claim that <m>20^\circ</m> is negative two times as hot as <m>-10^\circ</m>.
      </p>

      <p>
        Again, lets look at a more psychological example. Suppose I'm interested in looking at how the attitudes of first-year university students have changed over time. Obviously, I'm going to want to record the year in which each student started. This is an interval scale variable. A student who started in 2003 did arrive 5 years before a student who started in 2008. However, it would be completely insane for me to divide 2008 by 2003 and say that the second student started "1.0024 times later" than the first one. That doesn't make any sense at all.
      </p>
    </subsection>

    <subsection xml:id="subsubsec-ratio-scale">
      <title>Ratio scale</title>
      
      <p>
        The fourth and final type of variable to consider is a <term>ratio scale</term> variable, in which zero really means zero, and it's okay to multiply and divide. A good psychological example of a ratio scale variable is response time (RT). In a lot of tasks it's very common to record the amount of time somebody takes to solve a problem or answer a question, because it's an indicator of how difficult the task is. Suppose that Alan takes 2.3 seconds to respond to a question, whereas Ben takes 3.1 seconds. As with an interval scale variable, addition and subtraction are both meaningful here. Ben really did take <m>3.1 - 2.3 = 0.8</m> seconds longer than Alan did. However, notice that multiplication and division also make sense here too: Ben took <m>3.1 / 2.3 = 1.35</m> times as long as Alan did to answer the question. And the reason why you can do this is that, for a ratio scale variable such as RT, "zero seconds" really does mean "no time at all".
      </p>
    </subsection>

    <subsection xml:id="subsubsec-continuous-discrete">
      <title>Continuous versus discrete variables</title>
      
      <p>
        There's a second kind of distinction that you need to be aware of, regarding what types of variables you can run into. This is the distinction between continuous variables and discrete variables. The difference between these is as follows:
      </p>

      <ul>
        <li><p>A <term>continuous variable</term> is one in which, for any two values that you can think of, it's always logically possible to have another value in between.</p></li>
        <li><p>A <term>discrete variable</term> is, in effect, a variable that isn't continuous. For a discrete variable, it's sometimes the case that there's nothing in the middle.</p></li>
      </ul>

      <p>
        These definitions probably seem a bit abstract, but they're pretty simple once you see some examples. For instance, response time is continuous. If Alan takes 3.1 seconds and Ben takes 2.3 seconds to respond to a question, then it's possible for Cameron's response time to lie in between, by taking 3.0 seconds. And of course it would also be possible for David to take 3.031 seconds to respond, meaning that his RT would lie in between Cameron's and Alan's. And while in practice it might be impossible to measure RT that precisely, it's certainly possible in principle. Because we can always find a new value for RT in between any two other ones, we say that RT is continuous.
      </p>

      <p>
        Discrete variables occur when this rule is violated. For example, nominal scale variables are always discrete: there isn't a type of transportation that falls "in between" trains and bicycles, not in the strict mathematical way that 2.3 falls in between 2 and 3. So transportation type is discrete. Similarly, ordinal scale variables are always discrete: although "2nd place" does fall between "1st place" and "3rd place", there's nothing that can logically fall in between "1st place" and "2nd place". Interval scale and ratio scale variables can go either way. As we saw above, response time (a ratio scale variable) is continuous. Temperature in degrees Celsius (an interval scale variable) is also continuous. However, the year you went to school (an interval scale variable) is discrete. There's no year in between 2002 and 2003. The number of questions you get right on a true-or-false test (a ratio scale variable) is also discrete: since a true-or-false question doesn't allow you to be "partially correct", there's nothing in between 5/10 and 6/10. The table summarizes the relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible. I'm trying to hammer this point home, because (a) some textbooks get this wrong, and (b) people very often say things like "discrete variable" when they mean "nominal scale variable". It's very unfortunate.
      </p>

      <table xml:id="table-measurement-continuous-discrete">
        <title>The relationship between the scales of measurement and the discrete/continuity distinction. Cells with an x correspond to things that are possible.</title>
        <tabular>
          <row header="yes">
            <cell></cell>
            <cell halign="center">continuous</cell>
            <cell halign="center">discrete</cell>
          </row>
          <row>
            <cell>nominal</cell>
            <cell halign="center"></cell>
            <cell halign="center">x</cell>
          </row>
          <row>
            <cell>ordinal</cell>
            <cell halign="center"></cell>
            <cell halign="center">x</cell>
          </row>
          <row>
            <cell>interval</cell>
            <cell halign="center">x</cell>
            <cell halign="center">x</cell>
          </row>
          <row>
            <cell>ratio</cell>
            <cell halign="center">x</cell>
            <cell halign="center">x</cell>
          </row>
        </tabular>
      </table>
    </subsection>

    <subsection xml:id="subsubsec-scale-complexities">
      <title>Some complexities</title>
      
      <p>
        Okay, I know you're going to be shocked to hear this, but ...the real world is much messier than this little classification scheme suggests. Very few variables in real life actually fall into these nice neat categories, so you need to be kind of careful not to treat the scales of measurement as if they were hard and fast rules. It doesn't work like that: they're guidelines, intended to help you think about the situations in which you should treat different variables differently. Nothing more.
      </p>

      <p>
        So let's take a classic example, maybe <em>the</em> classic example, of a psychological measurement tool: the <term>Likert scale</term>. The humble Likert scale is the bread and butter tool of all survey design. You yourself have filled out hundreds, maybe thousands of them, and odds are you've even used one yourself. Suppose we have a survey question that looks like this:
      </p>

      <blockquote>
        <p>
          Which of the following best describes your opinion of the statement that "all pirates are freaking awesome" ...
        </p>
      </blockquote>

      <p>
        and then the options presented to the participant are these:
      </p>

      <blockquote>
        <p>
          (1) Strongly disagree<br />
          (2) Disagree<br />
          (3) Neither agree nor disagree<br />
          (4) Agree<br />
          (5) Strongly agree
        </p>
      </blockquote>

      <p>
        This set of items is an example of a 5-point Likert scale: people are asked to choose among one of several (in this case 5) clearly ordered possibilities, generally with a verbal descriptor given in each case. However, it's not necessary that all items be explicitly described. This is a perfectly good example of a 5-point Likert scale too:
      </p>

      <blockquote>
        <p>
          (1) Strongly disagree<br />
          (2)<br />
          (3)<br />
          (4)<br />
          (5) Strongly agree
        </p>
      </blockquote>

      <p>
        Likert scales are very handy, if somewhat limited, tools. The question is, what kind of variable are they? They're obviously discrete, since you can't give a response of 2.5. They're obviously not nominal scale, since the items are ordered; and they're not ratio scale either, since there's no natural zero.
      </p>

      <p>
        But are they ordinal scale or interval scale? One argument says that we can't really prove that the difference between "strongly agree" and "agree" is of the same size as the difference between "agree" and "neither agree nor disagree". In fact, in everyday life it's pretty obvious that they're not the same at all. So this suggests that we ought to treat Likert scales as ordinal variables. On the other hand, in practice most participants do seem to take the whole "on a scale from 1 to 5" part fairly seriously, and they tend to act as if the differences between the five response options were fairly similar to one another. As a consequence, a lot of researchers treat Likert scale data as if it were interval scale. It's not interval scale, but in practice it's close enough that we usually think of it as being <term>quasi-interval scale</term>.
      </p>
    </subsection>
  </section>

</chapter>
